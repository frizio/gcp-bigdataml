{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Machine Learning con TensorFlow </h1>\n",
    "\n",
    "In questa demo svilupperemo un modello di machine learning per predire la richiesta di taxi a New York in determinati giorni, utilizzando i dati pubblici sui viaggi in taxi a New York e sulle condizioni meteorologiche all'aeroporto di La Guardia che abbiamo usato anche nella demo precedente.\n",
    "\n",
    "Abbiamo tre obiettivi principali in questa demo\n",
    "\n",
    "1) avere un'idea di TensorFlow, come funziona, e quando usarlo\n",
    "\n",
    "2) fare ancora un po' di pratica con Datalab\n",
    "\n",
    "3) farsi un'idea del processo di analisi cui si sottopongono i dati nella creazione di modelli di Machine Learning\n",
    "\n",
    "Non si può diventare esperti di ML in cinque minuti, ma speriamo di darvi una buona introduzione e stimolarvi a volerne sapere di più!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se vogliamo inquadrare quello che stiamo facendo nel processo di sviluppo di modelli ML che ci ha spiegato Riccardo, a questo punto noi abbiamo terminato la fase 1, quella della raccolta di dati e analisi preliminare per stabilire feature e farci un'idea del modello che plausibilmente ci darà delle previsioni affidabili.\n",
    "\n",
    "Per riassumere quindi, presi dei dati pubblicamente disponibili sui viaggi in taxi a NY e informazioni meteorologiche all'aeroporto di La Guardia a NY, ci siamo cimentati nel capire possibili relazioni tra le varie feature con query in BigQuery, calcoli in Pandas, e grafici plottati all'interno di Datalab, e siamo così arrivati alla conclusione che il giorno della settimana e il tempo in un determinato giorno hanno un chiaro effetto sul numero di viaggi in taxi. Vogliamo quindi approfondire con Machine Learning, ripartendo da dove eravamo rimasti nella demo su BigQuery, e concludendo la prima fase di raccolta e analisi dei dati vista nella lezione con l'aggiunta di ancora più dati al nostro data frame: uniamo le statistiche anche per il 2014 e il 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importiamo le librerie necessarie per utilizzare BigQuery in Python e per la manipolazione avanzata dei dati in Python\n",
    "import google.datalab.bigquery as bq\n",
    "import pandas as pd # Pandas (strutture dati avanzate per l'analisi)\n",
    "import numpy as np # NumPy (libreria per calcoli scientifici)\n",
    "import shutil # operazioni su file e gruppi di file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bq query -n taxiquery\n",
    "# l'opzione -n crea una query con nome utilizzabile in Python come funzione\n",
    "WITH trips AS ( # WITH trips : crea una view temporanea utilizzabile in questo comando\n",
    "  SELECT EXTRACT (DAYOFYEAR from pickup_datetime) AS daynumber \n",
    "  FROM `bigquery-public-data.new_york.tlc_yellow_trips_*` # tabella partizionata per anno\n",
    "  where _TABLE_SUFFIX = @YEAR # variabile speciale utlizzata con tabelle partizionate\n",
    ")\n",
    "SELECT daynumber, COUNT(1) AS numtrips FROM trips # utilizza la view creata prima\n",
    "GROUP BY daynumber ORDER BY daynumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bq query -n wxquery\n",
    "# -n per una query con nome\n",
    "SELECT EXTRACT (DAYOFYEAR FROM CAST(CONCAT(@YEAR,'-',mo,'-',da) AS TIMESTAMP)) AS daynumber, # calcola il giorno dell'anno\n",
    "       EXTRACT (DAYOFWEEK FROM CAST(CONCAT(@YEAR,'-',mo,'-',da) AS TIMESTAMP)) AS dayofweek, # calcola il giorno della settimana\n",
    "       `min` AS mintemp, `max` AS maxtemp, IF(prcp=99.99,0,prcp) AS rain # backtick per chiarezza (min e max sono anche nomi di funzioni)\n",
    "FROM `bigquery-public-data.noaa_gsod.gsod*` # tabella partizionata con wildcard - il backtick è obbligatorio qui!\n",
    "WHERE stn='725030' AND _TABLE_SUFFIX = @YEAR # variabile speciale per il suffisso (_TABLE_SUFFIX) e parametro della query con nome (@YEAR)\n",
    "ORDER BY daynumber DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea un dataframe vuoto\n",
    "data2 = pd.DataFrame()\n",
    "# aggiungi i dati dei tre anni disponibili\n",
    "for year in [2014, 2015, 2016]:\n",
    "    query_parameters = [\n",
    "      {\n",
    "        'name': 'YEAR',\n",
    "        'parameterType': {'type': 'STRING'},\n",
    "        'parameterValue': {'value': year}\n",
    "      }\n",
    "    ]\n",
    "    weather = wxquery.execute(query_params=query_parameters).result().to_dataframe()\n",
    "    trips = taxiquery.execute(query_params=query_parameters).result().to_dataframe()\n",
    "    data_for_year = pd.merge(weather, trips, on='daynumber') # unisci dati sui viaggi in taxi e dati metereologici\n",
    "    data2 = pd.concat([data2, data_for_year]) # aggiungi l'anno in elaborazione al dataframe finale\n",
    "# mostra i primi 15 valori del dataframe finale\n",
    "data2[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per preparare i dati all'addestramento del modello, per prima cosa li mischiamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mischiamo gli elementi del data frame a caso\n",
    "# NB frac=1 significa che li teniamo tutti\n",
    "shuffled = data2.sample(frac=1)\n",
    "# ora mostra i primi cinqui dati mischiati\n",
    "shuffled[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poi mettiamo da parte i dati che non ci servono (il giorno dell'anno in questo caso, visto che non è rilevante per stabilire il numero di viaggi in taxi) e il target delle nostre predizioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prendiamo solo le colonne dalla terza alla quinta,\n",
    "# escludendo quindi la prima, la seconda e l'ultima\n",
    "predictors = shuffled.iloc[:,1:5]\n",
    "# one-hot encoding del giorno della settimana\n",
    "# NB ci bastano sei campi: il settimo giorno è\n",
    "# codificato come tutti zero nei sei campi\n",
    "# predictors = shuffled.iloc[:,2:5]\n",
    "# for day in range(1,7):\n",
    "#   matching = shuffled['dayofweek'] == day\n",
    "#   key = 'day_' + str(day)\n",
    "#   predictors[key] = pd.Series(matching, index=predictors.index, dtype=float)\n",
    "# mostra i primi cinque dati per le predizioni di test\n",
    "predictors[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In un'altra variabile mettiamo invece il valore dei viaggi in taxi effettuati. Questo è il risultato che cerchiamo di ottenere con il modello. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I valori che ci aspettiamo come risultato\n",
    "targets = shuffled.iloc[:,5]\n",
    "# Mostra i primi 5 target\n",
    "targets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> TensorFlow </h2>\n",
    "\n",
    "Importiamo TensorFlow e configuriamo alcune variabili che useremo nell'addestramento dei modelli e per la predizione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importa la libreria di TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# la scala del valore da predire\n",
    "# NB è meglio tenere il lavoro da predire più vicino possibile\n",
    "# al range 0.0-1.0, per cui diamo la scala dei valori per\n",
    "# poter dividere i valori che ci attendiamo\n",
    "SCALE_NUM_TRIPS = 600000\n",
    "\n",
    "# il numero totale di dati per l'addestramento\n",
    "# NB in questo semplice esempio non faremo del testing\n",
    "trainsize = len(targets)\n",
    "\n",
    "# il numero di feature in input\n",
    "npredictors = len(predictors.columns)\n",
    "\n",
    "# il numero di variabili che il modello predice\n",
    "noutputs = 1\n",
    "\n",
    "# impostiamo il livello di logging\n",
    "# NB mettete INFO per ricevere output ogni 100 step\n",
    "tf.logging.set_verbosity(tf.logging.WARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Regressione lineare </h2>\n",
    "\n",
    "Utilizziamo un modello di regressione lineare per predire il numero di viaggi in taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminiamo l'intera directory dell'eventuale modello precedente\n",
    "shutil.rmtree('./trained_model_linear', ignore_errors=True)\n",
    "\n",
    "# crea lo stimatore, usato per addestrare e fare predizioni\n",
    "estimator = tf.contrib.learn.LinearRegressor(\n",
    "  model_dir='./trained_model_linear',\n",
    "  feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)\n",
    ")\n",
    "\n",
    "# addestra il modello\n",
    "print(\"Starting to train ... this will take a while ... use verbosity=INFO to get more verbose output\")\n",
    "def input_fn(features, targets):\n",
    "  return tf.constant(features.values), tf.constant(targets.values.reshape(len(targets), noutputs) / SCALE_NUM_TRIPS)\n",
    "estimator.fit(input_fn=lambda: input_fn(predictors[:trainsize], targets[:trainsize]), steps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Rete neurale </h2>\n",
    "\n",
    "Utilizziamo una rete neurale per predire il numero di viaggi in taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminiamo l'intera directory dell'eventuale modello precedente\n",
    "shutil.rmtree('./trained_model', ignore_errors=True)\n",
    "\n",
    "# crea lo stimatore, usato per addestrare e fare predizioni\n",
    "estimator = tf.contrib.learn.DNNRegressor(\n",
    "  model_dir='./trained_model',\n",
    "  hidden_units=[5, 5],                             \n",
    "  feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)\n",
    ")\n",
    "\n",
    "# addestra il modello\n",
    "print(\"starting to train ... this will take a while ... use verbosity=INFO to get more verbose output\")\n",
    "def input_fn(features, targets):\n",
    "  return tf.constant(features.values), tf.constant(targets.values.reshape(len(targets), noutputs))\n",
    "estimator.fit(input_fn=lambda: input_fn(predictors[:trainsize], targets[:trainsize]), steps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Fare predizioni </h2>\n",
    "\n",
    "Ora possiamo usare i modelli per fare delle predizioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature in input per calcolare la predizione\n",
    "input = pd.DataFrame.from_dict(data = \n",
    "                               {\n",
    "                                'dayofweek': [1, 3, 6],\n",
    "                                'mintemp' : [60, 40, 50],\n",
    "                                'maxtemp' : [70, 90, 60],\n",
    "                                'rain' : [0, 0.5, 0]\n",
    "                               }\n",
    "                              )\n",
    "\n",
    "# apri i modello dalla cartella dove sono stati creati\n",
    "estimator1 = tf.contrib.learn.DNNRegressor(\n",
    "  model_dir='./trained_model',\n",
    "  hidden_units=[5, 5],\n",
    "  feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(input.values)\n",
    ")\n",
    "estimator2 = tf.contrib.learn.LinearRegressor(\n",
    "  model_dir='./trained_model_linear',\n",
    "  feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(input.values)\n",
    ")\n",
    "\n",
    "# passa le feature e ottieni le predizioni\n",
    "pred1 = np.multiply(list(estimator1.predict(input.values)), 1.0)\n",
    "pred2 = np.multiply(list(estimator2.predict(input.values)), SCALE_NUM_TRIPS)\n",
    "print(\"DNN=\", pred1)\n",
    "print(\"Linear=\", pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License</em>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
