{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Machine Learning con Tensorflow </h1>\n",
    "\n",
    "In questa demo svilupperemo un modello di machine learning per predirre la richiesta di taxi a New York in determinati giorni, utilizzando i dati pubblici sui viaggi in taxi a New York e sulle condizioni metereologiche all'aereoporto di La Guardia che abbiamo usato anche nella demo precedente.\n",
    "\n",
    "Abbiamo tre obiettivi principali in questa demo\n",
    "\n",
    "1) avere un'idea di TensorFlow, come funziona, e quando usarlo\n",
    "\n",
    "2) fare ancora un po' di pratica con Datalab\n",
    "\n",
    "3) farsi un'idea del processo di analisi cui si sottopongono i dati nella creazione di modelli di Machine Learning\n",
    "\n",
    "Non si può diventare esperti di ML in cinque minuti, ma speriamo di darvi una buona introduzione e stimolarvi a volerne sapere di più!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se vogliamo inquadrare quello che stiamo facendo nel processo di sviluppo di modelli ML che ci ha spiegato Riccardo, a questo punto noi abbiamo terminato la fase 1, quella della raccolta di dati e analisi preliminare per stabilire feature e farci un'idea del modello che plausibilmente ci darà delle previsioni affidabili.\n",
    "\n",
    "Per riassumere quindi, presi dei dati pubblicamente disponibili sui viaggi in taxi a NY e informazioni metereologiche all'aeroporto di La Guardia a NY, ci siamo cimentati nel capire possibili relazioni tra le varie feature con query in BigQuery, calcoli in Pandas, e grafici plottati all'interno di Datalab, e siamo così arrivati alla conclusione che il giorno della settimana e il tempo in un determinato giorno hanno un chiaro effetto sul numero di viaggi in taxi. Vogliamo quindi approfondire con Machine Learning, ripartendo da dove eravamo rimasti nella demo su BigQuery, e concludendo la prima fase di raccolta e analisi dei dati vista nella lezione con l'aggiunta di ancora più dati al nostro dataframe: uniamo le statistiche anche per il 2014 e il 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importiamo le librerie necessarie per utilizzare BigQuery in Python e per la manipolazione avanzata dei dati in Python\n",
    "import google.datalab.bigquery as bq\n",
    "import pandas as pd # Pandas (strutture dati avanzate per l'analisi)\n",
    "import numpy as np # NumPy (libreria per calcoli scientifici)\n",
    "import shutil # operazioni su file e gruppi di file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bq query -n taxiquery\n",
    "# l'opzione -n crea una query con nome utilizzabile in Python come funzione\n",
    "WITH trips AS ( # WITH trips : crea una view temporanea utilizzabile in questo comando\n",
    "  SELECT EXTRACT (DAYOFYEAR from pickup_datetime) AS daynumber \n",
    "  FROM `bigquery-public-data.new_york.tlc_yellow_trips_*` # tabella partizionata per anno\n",
    "  where _TABLE_SUFFIX = @YEAR # variabile speciale utlizzata con tabelle partizionate\n",
    ")\n",
    "SELECT daynumber, COUNT(1) AS numtrips FROM trips # utilizza la view creata prima\n",
    "GROUP BY daynumber ORDER BY daynumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bq query -n wxquery\n",
    "# -n per una query con nome\n",
    "SELECT EXTRACT (DAYOFYEAR FROM CAST(CONCAT(@YEAR,'-',mo,'-',da) AS TIMESTAMP)) AS daynumber, # calcola il giorno dell'anno\n",
    "       EXTRACT (DAYOFWEEK FROM CAST(CONCAT(@YEAR,'-',mo,'-',da) AS TIMESTAMP)) AS dayofweek, # calcola il giorno della settimana\n",
    "       `min` AS mintemp, `max` AS maxtemp, IF(prcp=99.99,0,prcp) AS rain # backtick per chiarezza (min e max sono anche nomi di funzioni)\n",
    "FROM `bigquery-public-data.noaa_gsod.gsod*` # tabella partizionata con wildcard - il backtick è obbligatorio qui!\n",
    "WHERE stn='725030' AND _TABLE_SUFFIX = @YEAR # variabile speciale per il suffisso (_TABLE_SUFFIX) e parametro della query con nome (@YEAR)\n",
    "ORDER BY daynumber DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea un dataframe vuoto\n",
    "data2 = pd.DataFrame()\n",
    "# aggiungi i dati dei tre anni disponibili\n",
    "for year in [2014, 2015, 2016]:\n",
    "    query_parameters = [\n",
    "      {\n",
    "        'name': 'YEAR',\n",
    "        'parameterType': {'type': 'STRING'},\n",
    "        'parameterValue': {'value': year}\n",
    "      }\n",
    "    ]\n",
    "    weather = wxquery.execute(query_params=query_parameters).result().to_dataframe()\n",
    "    trips = taxiquery.execute(query_params=query_parameters).result().to_dataframe()\n",
    "    data_for_year = pd.merge(weather, trips, on='daynumber') # unisci dati sui viaggi in taxi e dati metereologici\n",
    "    data2 = pd.concat([data2, data_for_year]) # aggiungi l'anno in elaborazione al dataframe finale\n",
    "# mostra i primi 15 valori del dataframe finale\n",
    "data2[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come Riccardo ci ha spiegato, nell'addestramento di un modello solitamente si dividono i dati in training data e test data. Noi utilizzeremo l'80% per l'addestramento e il 20% per il testing.\n",
    "\n",
    "We'll use 80% of our dataset for training and 20% of the data for testing the model we have trained. Let's shuffle the rows of the Pandas dataframe so that this division is random.  The predictor (or input) columns will be every column in the database other than the number-of-trips (which is our target, or what we want to predict).\n",
    "\n",
    "The machine learning models that we will use -- linear regression and neural networks -- both require that the input variables are numeric in nature.\n",
    "\n",
    "The day of the week, however, is a categorical variable (i.e. Tuesday is not really greater than Monday). So, we should create separate columns for whether it is a Monday (with values 0 or 1), Tuesday, etc.\n",
    "\n",
    "Against that, we do have limited data (remember: the more columns you use as input features, the more rows you need to have in your training dataset), and it appears that there is a clear linear trend by day of the week. So, we will opt for simplicity here and use the data as-is.  Try uncommenting the code that creates separate columns for the days of the week and re-run the notebook if you are curious about the impact of this simplification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "shuffled = data2.sample(frac=1, random_state=13)\n",
    "# It would be a good idea, if we had more data, to treat the days as categorical variables\n",
    "# with the small amount of data, we have though, the model tends to overfit\n",
    "#predictors = shuffled.iloc[:,2:5]\n",
    "#for day in range(1,8):\n",
    "#  matching = shuffled['dayofweek'] == day\n",
    "#  key = 'day_' + str(day)\n",
    "#  predictors[key] = pd.Series(matching, index=predictors.index, dtype=float)\n",
    "predictors = shuffled.iloc[:,1:5]\n",
    "predictors[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = shuffled.iloc[:,5]\n",
    "targets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update our benchmark based on the 80-20 split and the larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsize = int(len(shuffled['numtrips']) * 0.8)\n",
    "avg = np.mean(shuffled['numtrips'][:trainsize])\n",
    "rmse = np.sqrt(np.mean((targets[trainsize:] - avg)**2))\n",
    "print('Just using average={0} has RMSE of {1}'.format(avg, rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Linear regression with tf.contrib.learn </h2>\n",
    "\n",
    "We scale the number of taxicab rides by 400,000 so that the model can keep its predicted values in the [0-1] range. The optimization goes a lot faster when the weights are small numbers.  We save the weights into ./trained_model_linear and display the root mean square error on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_NUM_TRIPS = 600000.0\n",
    "trainsize = int(len(shuffled['numtrips']) * 0.8)\n",
    "testsize = len(shuffled['numtrips']) - trainsize\n",
    "npredictors = len(predictors.columns)\n",
    "noutputs = 1\n",
    "tf.logging.set_verbosity(tf.logging.WARN) # change to INFO to get output every 100 steps ...\n",
    "shutil.rmtree('./trained_model_linear', ignore_errors=True) # so that we don't load weights from previous runs\n",
    "estimator = tf.contrib.learn.LinearRegressor(model_dir='./trained_model_linear',\n",
    "                                             feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values))\n",
    "\n",
    "print(\"starting to train ... this will take a while ... use verbosity=INFO to get more verbose output\")\n",
    "def input_fn(features, targets):\n",
    "  return tf.constant(features.values), tf.constant(targets.values.reshape(len(targets), noutputs)/SCALE_NUM_TRIPS)\n",
    "estimator.fit(input_fn=lambda: input_fn(predictors[:trainsize], targets[:trainsize]), steps=10000)\n",
    "\n",
    "pred = np.multiply(list(estimator.predict(predictors[trainsize:].values)), SCALE_NUM_TRIPS )\n",
    "rmse = np.sqrt(np.mean(np.power((targets[trainsize:].values - pred), 2)))\n",
    "print('LinearRegression has RMSE of {0}'.format(rmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE here (57K) is lower than the benchmark (62K) indicates that we are doing about 10% better with the machine learning model than we would be if we were to just use the historical average (our benchmark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Neural network with tf.contrib.learn </h2>\n",
    "\n",
    "Let's make a more complex model with a few hidden nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_NUM_TRIPS = 600000.0\n",
    "trainsize = int(len(shuffled['numtrips']) * 0.8)\n",
    "testsize = len(shuffled['numtrips']) - trainsize\n",
    "npredictors = len(predictors.columns)\n",
    "noutputs = 1\n",
    "tf.logging.set_verbosity(tf.logging.WARN) # change to INFO to get output every 100 steps ...\n",
    "shutil.rmtree('./trained_model', ignore_errors=True) # so that we don't load weights from previous runs\n",
    "estimator = tf.contrib.learn.DNNRegressor(model_dir='./trained_model',\n",
    "                                          hidden_units=[5, 5],                             \n",
    "                                          feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values))\n",
    "\n",
    "print(\"starting to train ... this will take a while ... use verbosity=INFO to get more verbose output\")\n",
    "def input_fn(features, targets):\n",
    "  return tf.constant(features.values), tf.constant(targets.values.reshape(len(targets), noutputs)/SCALE_NUM_TRIPS)\n",
    "estimator.fit(input_fn=lambda: input_fn(predictors[:trainsize], targets[:trainsize]), steps=10000)\n",
    "\n",
    "pred = np.multiply(list(estimator.predict(predictors[trainsize:].values)), SCALE_NUM_TRIPS )\n",
    "rmse = np.sqrt(np.mean((targets[trainsize:].values - pred)**2))\n",
    "print('Neural Network Regression has RMSE of {0}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a neural network results in similar performance to the linear model when I ran it -- it might be because there isn't enough data for the NN to do much better. (NN training is a non-convex optimization, and you will get different results each time you run the above code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Running a trained model </h2>\n",
    "\n",
    "So, we have trained a model, and saved it to a file. Let's use this model to predict taxicab demand given the expected weather for three days.\n",
    "\n",
    "Here we make a Dataframe out of those inputs, load up the saved model (note that we have to know the model equation -- it's not saved in the model file) and use it to predict the taxicab demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = pd.DataFrame.from_dict(data = \n",
    "                               {'dayofweek' : [4, 5, 6],\n",
    "                                'mintemp' : [60, 40, 50],\n",
    "                                'maxtemp' : [70, 90, 60],\n",
    "                                'rain' : [0, 0.5, 0]})\n",
    "# read trained model from ./trained_model\n",
    "estimator = tf.contrib.learn.LinearRegressor(model_dir='./trained_model_linear',\n",
    "                                          feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(input.values))\n",
    "\n",
    "pred = np.multiply(list(estimator.predict(input.values)), SCALE_NUM_TRIPS )\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
